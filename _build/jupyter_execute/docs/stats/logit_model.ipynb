{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ロジットモデル (logit model)\n",
    "---\n",
    "- ロジットモデルは、**ダミー変数などの2値変数 (0, 1の値をとる) を目的変数**として、回帰分析を適用する手法のひとつである。\n",
    "    - ロジスティック回帰（Logistic Regression）やロジット回帰（Logit　Regression）とも言われる。\n",
    "- モデルの出力値をどちらかのカテゴリに所属する確率と見なすことで、**カテゴリ予測**を可能にする。\n",
    "    - 以下の例のように、各データがカテゴリA, Bのどちらに所属するかを$A=0, B=1$の2値で表し、予測を行う。\n",
    "<!--\n",
    "<table class=\"background-bright border text-center\" style=\"text-align: center\">\n",
    "    <tr class=\"background-dark\">\n",
    "        <th></th>\n",
    "        <th>$x_1$</th>\n",
    "        <th>$x_2$</th>\n",
    "        <th>$\\dots$</th>\n",
    "        <th>$x_k$</th>\n",
    "        <th class=\"border-right-double\" style=\"min-width: 11em;\">$y\\ (A=1,\\ B=0)$</th>\n",
    "        <th>$\\hat{y}$</th>\n",
    "        <th>予測結果</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$1$</td>\n",
    "        <td class=\"text-right\">1.0</td>\n",
    "        <td class=\"text-right\">-2.0</td>\n",
    "        <td>$\\dots$</td>\n",
    "        <td class=\"text-right\">3.0</td>\n",
    "        <td class=\"border-right-double\">1</td>\n",
    "        <td>0.98 (A:98%, B: 2%)</td>\n",
    "        <td>A</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$2$</td>\n",
    "        <td class=\"text-right\">-2.5</td>\n",
    "        <td class=\"text-right\">1.3</td>\n",
    "        <td>$\\dots$</td>\n",
    "        <td class=\"text-right\">1.1</td>\n",
    "        <td class=\"border-right-double\">0</td>\n",
    "        <td>0.52 (A:52%, B:48%)</td>\n",
    "        <td>A</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=\"1\">$\\vdots$</td>\n",
    "        <td colspan=\"1\">$\\vdots$</td>\n",
    "        <td colspan=\"1\">$\\vdots$</td>\n",
    "        <td colspan=\"1\"></td>\n",
    "        <td colspan=\"1\">$\\vdots$</td>\n",
    "        <td colspan=\"1\">$\\vdots$</td>\n",
    "        <td colspan=\"1\">$\\vdots$</td>\n",
    "        <td colspan=\"1\">$\\vdots$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>$n$</td>\n",
    "        <td class=\"text-right\">3.3</td>\n",
    "        <td class=\"text-right\">0.9</td>\n",
    "        <td>$\\dots$</td>\n",
    "        <td class=\"text-right\">-0.5</td>\n",
    "        <td class=\"border-right-double\">0</td>\n",
    "        <td>0.36 (A:36%, B:64%)</td>\n",
    "        <td>B</td>\n",
    "    </tr>\n",
    "</table>\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ロジットモデルの例](./image/logit_01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 線形回帰との関係\n",
    "---\n",
    "- ロジットモデルでは、 $z=a+b_{1}x_{1}+b_{2}x_{2}+\\dots+b_{k}x_{k}$ とすると 出力（予測結果）$\\hat{y}$ は以下の式から求める。\n",
    "    - $z$は、線形回帰 (単回帰・重回帰) の出力である。\n",
    "    - 線形回帰の出力にロジスティック関数 $\\Lambda (x)={\\displaystyle \\frac{e^{x}}{1+e^{x}}}$ を適用したものが$\\hat{y}$である。\n",
    "\n",
    "$$\n",
    "\\hat{y} ={\\displaystyle \\frac{e^{z}}{1+e^{z}}} \\left( ={\\displaystyle \\frac{1}{1+e^{-z}}} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "- ロジスティック関数は、以下を満たす累積分布関数である。\n",
    "    - $-\\infty<x<\\infty$ の区間で $0<\\Lambda(x)<1$\n",
    "    - $\\Lambda (x)$ を微分した導関数 $\\Lambda '(x)=\\Lambda(x)\\left(1-\\Lambda(x)\\right)$ は、 $\\Lambda'(x)>0$ (確率密度の条件)\n",
    "\n",
    "\n",
    "- ロジスティック関数が表す確率分布はロジスティック分布と呼ばれる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ロジスティック分布](./image/logit_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 線形回帰の出力を確率分布に変換する累積分布関数は、あらゆる実数の入力 ( $-\\infty<x<\\infty$ ) に対応する確率を出力すれば何でも良い。\n",
    "- しかし、一般的にはロジスティック分布 (ロジットモデル) か標準正規分布 (プロビットモデル) が使われる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景 (読み飛ばし可)\n",
    "※数学好きな人向け"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回帰式の意味\n",
    "---\n",
    "$y_{i}=1$ となる確率 $p_{i}( X_{i})$ とそれ以外の確率 $1-p_{i}( X_{i})$ の比 (オッズ比) $O_{i}( X_{i})$ を\n",
    "\n",
    "$$\n",
    "O_{i}( X_{i})={\\displaystyle \\frac{p_{i}( X_{i})}{1-p_{i}( X_{i})}}\n",
    "$$\n",
    "\n",
    "として、これを $p_{i}( X_{i})$ について整理すると\n",
    "\n",
    "$$\n",
    "p_{i}( X_{i}) ={\\displaystyle \\frac{O_{i}( X_{i})}{1+O_{i}( X_{i})}} \n",
    "$$\n",
    "\n",
    "となり、 $p_{i}( X_{i})$ の予測 $\\hat{y_{i}}={\\displaystyle \\frac{e^{z_{i}}}{1+e^{z_{i}}}}$ と一致する。\n",
    "\n",
    "\n",
    "\n",
    "つまり、 $e^{z_{i}}$ は $y_{i}=1$ となる確率のオッズ比であり、ロジスティック回帰で求めた $z_{i}$ は対数オッズ比 $logO_{i}( X_{i})$ と解釈できる。\n",
    "<!--\n",
    "<table class=\"border text-center background-bright\">\n",
    "    <tr class=\"background-dark\">\n",
    "        <th></th>\n",
    "        <th>誤差項の分布</th>\n",
    "        <th>特徴</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th class=\"background-dark border-bottom\">ロジットモデル<br />(ロジスティック回帰)</th>\n",
    "        <td>ロジスティック分布</td>\n",
    "        <td class=\"text-left\">累積分布関数が計算しやすい<br />回帰式の係数の意味が解釈しやすい (オッズ比)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th class=\"background-dark border-bottom\">プロビットモデル</th>\n",
    "        <td>標準正規分布</td>\n",
    "        <td class=\"text-left\">回帰分析の考え方と親和的である</td>\n",
    "    </tr>\n",
    "</table>\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ロジットモデルとプロピットモデルの比較](./image/logit_03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 誤差の分布\n",
    "---\n",
    "線形回帰と同じように誤差項 $\\epsilon$ を使って、 $A_{i} =a+b_{1} x_{i1} +b_{2} x_{i2} +\\dots +b_{k} x_{ik} +\\epsilon _{i}$ とすると、\n",
    "\n",
    "$$\n",
    "y_{i} =\\left\\{\\begin{aligned}\n",
    "    1 &  & A_{i} +\\epsilon _{i} =a+b_{1} x_{i1} +b_{2} x_{i2} +\\dots +b_{k} x_{ik} +\\epsilon _{i}  >0 & , & F( A_{i} +\\epsilon _{i})  >0.5\\\\\n",
    "    0 &  & A_{i} +\\epsilon _{i} =a+b_{1} x_{i1} +b_{2} x_{i2} +\\dots +b_{k} x_{ik} +\\epsilon _{i} \\leqq 0 & , & F( A_{i} +\\epsilon _{i}) \\leqq 0.5\n",
    "\\end{aligned}\\right.\n",
    "$$\n",
    "\n",
    "このとき、ロジットモデルは誤差項 $\\epsilon$ の分布にロジスティック分布を仮定している。 (プロビットモデルは正規分布)\n",
    "\n",
    "ロジットモデルやプロビットモデルのように誤差項の分布とそれに対応する関数を用いて $y$ を $x$ と $\\epsilon $ の線型結合に分解して分析する手法を一般化線形モデル (generalized linear model) と呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パラメーターの求め方\n",
    "---\n",
    "- 通常、最尤法 (maximum likelihood method)、最尤推定 (maximum likelihood estimation, MLE) を用いる。\n",
    "- 最尤法とは、関数 $F(x)$ から元のデータ $(X_i,\\ y_i)\\ (i=1,\\ 2,\\dots,\\ n)$ が再現される確率 (尤度) を最大化するようにパラメーターを決める方法である。\n",
    "\n",
    "$$\n",
    "{\\displaystyle 尤度=p( y_{1} |x_{1}) \\cdot p( y_{2} |x_{2}) \\dotsc p( y_{n} |x_{n}) =\\prod ^{n}_{i=1} p( y_{i} |x_{i})}\n",
    "$$\n",
    "\n",
    "- 累積分布関数を $F(x)$ とし、あるデータ $X_{i}\\ (x_{1} ,\\ x_{2} ,\\dots,\\ x_{n} )$ が与えられたときに $y_{i} =1$ となる確率を $P(y_{i} =1\\ |\\ X_{i} )=F(a+b_{1} x_{i1} +b_{2} x_{i2}+\\dots +b_{k} x_{ik})$ と表すと、 $P(y_{i} =0\\ |\\ X_{i})=1-P(y_{i} =1\\ |\\ X_{i} )$ なので、尤度は以下の尤度関数 $L$ で表わすことができる。\n",
    "\n",
    "$$\n",
    "L( a,\\ b_{1} ,\\ b_{2} ,\\dots ,\\ b_{k}) ={\\displaystyle \\prod _{\\{i\\ |\\ y_{i} =1\\}} F( a+b_{1} x_{i1} +b_{2} x_{i2} +\\dots +b_{k} x_{ik}) \\cdot \\prod _{\\{i\\ |\\ y_{i} =0\\}}[ 1-F( a+b_{1} x_{i1} +b_{2} x_{i2} +\\dots +b_{k} x_{ik})]} \n",
    "$$\n",
    "\n",
    "- 確率の積の形は、値が小さすぎて、コンピュータで計算しにくいため、対数をとる。\n",
    "\n",
    "$$\n",
    "log\\ L( a,\\ b_{1} ,\\ b_{2} ,\\dots ,\\ b_{k}) ={\\displaystyle \\sum _{\\{i\\ |\\ y_{i} =1\\}} log\\ F( a+b_{1} x_{i1} +b_{2} x_{i2} +\\dots +b_{k} x_{ik}) +\\sum _{\\{i\\ |\\ y_{i} =0\\}}[ 1-F( a+b_{1} x_{i1} +b_{2} x_{i2} +\\dots +b_{k} x_{ik})]} \n",
    "$$\n",
    "\n",
    "- $y_{i} =1$ のとき、 $1-y_{i} =0$、 $y_{i} =0$ のとき、 $1-y_{i} =1$ であるため、上式は下式のようにまとめられる。\n",
    "\n",
    "$$\n",
    "log\\ L( a,\\ b_{1} ,\\ b_{2} ,\\dots ,\\ b_{k}) ={\\displaystyle \\sum ^{n}_{i=1}\\{y_{i} \\ log\\ F( a+b_{1} x_{i1} +b_{2} x_{i2} +\\dots +b_{k} x_{ik}) +( 1-y_{i}) log\\ [ 1-F( a+b_{1} x_{i1} +b_{2} x_{i2} +\\dots +b_{k} x_{ik})]\\}} \n",
    "$$\n",
    "\n",
    "- この対数尤度 $log\\ L$ を最大化するパラメーター $a,\\ b_{1} ,\\ b_{2} ,\\dots ,\\ b_{k}$ を母数の推定値とするのが最尤法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パラメーターの標本分布\n",
    "---\n",
    "- パラメーターの標本分布を求めるのは困難なので、通常は中心極限定理によって標準分布に近似する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パラメーターの検定\n",
    "---\n",
    "- 自由度 $n-(k+1)$ の $t$ 分布を利用して $t$ 検定を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythonでの実行方法\n",
    "---\n",
    "- statsmodels.discrete.discrete_model.Logitを用いる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Logit in module statsmodels.discrete.discrete_model:\n",
      "\n",
      "class Logit(BinaryModel)\n",
      " |  Logit(endog, exog, check_rank=True, **kwargs)\n",
      " |  \n",
      " |  Logit Model\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  endog : array_like\n",
      " |      A 1-d endogenous response variable. The dependent variable.\n",
      " |  exog : array_like\n",
      " |      A nobs x k array where `nobs` is the number of observations and `k`\n",
      " |      is the number of regressors. An intercept is not included by default\n",
      " |      and should be added by the user. See\n",
      " |      :func:`statsmodels.tools.add_constant`.\n",
      " |  missing : str\n",
      " |      Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n",
      " |      checking is done. If 'drop', any observations with nans are dropped.\n",
      " |      If 'raise', an error is raised. Default is 'none'.\n",
      " |  check_rank : bool\n",
      " |      Check exog rank to determine model degrees of freedom. Default is\n",
      " |      True. Setting to False reduces model initialization time when\n",
      " |      exog.shape[1] is large.\n",
      " |  \n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  endog : ndarray\n",
      " |      A reference to the endogenous response variable\n",
      " |  exog : ndarray\n",
      " |      A reference to the exogenous design.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Logit\n",
      " |      BinaryModel\n",
      " |      DiscreteModel\n",
      " |      statsmodels.base.model.LikelihoodModel\n",
      " |      statsmodels.base.model.Model\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  cdf(self, X)\n",
      " |      The logistic cumulative distribution function\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          `X` is the linear predictor of the logit model.  See notes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      1/(1 + exp(-X))\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      In the logit model,\n",
      " |      \n",
      " |      .. math:: \\Lambda\\left(x^{\\prime}\\beta\\right)=\n",
      " |                \\text{Prob}\\left(Y=1|x\\right)=\n",
      " |                \\frac{e^{x^{\\prime}\\beta}}{1+e^{x^{\\prime}\\beta}}\n",
      " |  \n",
      " |  fit(self, start_params=None, method='newton', maxiter=35, full_output=1, disp=1, callback=None, **kwargs)\n",
      " |      Fit the model using maximum likelihood.\n",
      " |      \n",
      " |      The rest of the docstring is from\n",
      " |      statsmodels.base.model.LikelihoodModel.fit\n",
      " |      \n",
      " |      Fit method for likelihood based models\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      start_params : array_like, optional\n",
      " |          Initial guess of the solution for the loglikelihood maximization.\n",
      " |          The default is an array of zeros.\n",
      " |      method : str, optional\n",
      " |          The `method` determines which solver from `scipy.optimize`\n",
      " |          is used, and it can be chosen from among the following strings:\n",
      " |      \n",
      " |          - 'newton' for Newton-Raphson, 'nm' for Nelder-Mead\n",
      " |          - 'bfgs' for Broyden-Fletcher-Goldfarb-Shanno (BFGS)\n",
      " |          - 'lbfgs' for limited-memory BFGS with optional box constraints\n",
      " |          - 'powell' for modified Powell's method\n",
      " |          - 'cg' for conjugate gradient\n",
      " |          - 'ncg' for Newton-conjugate gradient\n",
      " |          - 'basinhopping' for global basin-hopping solver\n",
      " |          - 'minimize' for generic wrapper of scipy minimize (BFGS by default)\n",
      " |      \n",
      " |          The explicit arguments in `fit` are passed to the solver,\n",
      " |          with the exception of the basin-hopping solver. Each\n",
      " |          solver has several optional arguments that are not the same across\n",
      " |          solvers. See the notes section below (or scipy.optimize) for the\n",
      " |          available arguments and for the list of explicit arguments that the\n",
      " |          basin-hopping solver supports.\n",
      " |      maxiter : int, optional\n",
      " |          The maximum number of iterations to perform.\n",
      " |      full_output : bool, optional\n",
      " |          Set to True to have all available output in the Results object's\n",
      " |          mle_retvals attribute. The output is dependent on the solver.\n",
      " |          See LikelihoodModelResults notes section for more information.\n",
      " |      disp : bool, optional\n",
      " |          Set to True to print convergence messages.\n",
      " |      fargs : tuple, optional\n",
      " |          Extra arguments passed to the likelihood function, i.e.,\n",
      " |          loglike(x,*args)\n",
      " |      callback : callable callback(xk), optional\n",
      " |          Called after each iteration, as callback(xk), where xk is the\n",
      " |          current parameter vector.\n",
      " |      retall : bool, optional\n",
      " |          Set to True to return list of solutions at each iteration.\n",
      " |          Available in Results object's mle_retvals attribute.\n",
      " |      skip_hessian : bool, optional\n",
      " |          If False (default), then the negative inverse hessian is calculated\n",
      " |          after the optimization. If True, then the hessian will not be\n",
      " |          calculated. However, it will be available in methods that use the\n",
      " |          hessian in the optimization (currently only with `\"newton\"`).\n",
      " |      kwargs : keywords\n",
      " |          All kwargs are passed to the chosen solver with one exception. The\n",
      " |          following keyword controls what happens after the fit::\n",
      " |      \n",
      " |              warn_convergence : bool, optional\n",
      " |                  If True, checks the model for the converged flag. If the\n",
      " |                  converged flag is False, a ConvergenceWarning is issued.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The 'basinhopping' solver ignores `maxiter`, `retall`, `full_output`\n",
      " |      explicit arguments.\n",
      " |      \n",
      " |      Optional arguments for solvers (see returned Results.mle_settings)::\n",
      " |      \n",
      " |          'newton'\n",
      " |              tol : float\n",
      " |                  Relative error in params acceptable for convergence.\n",
      " |          'nm' -- Nelder Mead\n",
      " |              xtol : float\n",
      " |                  Relative error in params acceptable for convergence\n",
      " |              ftol : float\n",
      " |                  Relative error in loglike(params) acceptable for\n",
      " |                  convergence\n",
      " |              maxfun : int\n",
      " |                  Maximum number of function evaluations to make.\n",
      " |          'bfgs'\n",
      " |              gtol : float\n",
      " |                  Stop when norm of gradient is less than gtol.\n",
      " |              norm : float\n",
      " |                  Order of norm (np.Inf is max, -np.Inf is min)\n",
      " |              epsilon\n",
      " |                  If fprime is approximated, use this value for the step\n",
      " |                  size. Only relevant if LikelihoodModel.score is None.\n",
      " |          'lbfgs'\n",
      " |              m : int\n",
      " |                  This many terms are used for the Hessian approximation.\n",
      " |              factr : float\n",
      " |                  A stop condition that is a variant of relative error.\n",
      " |              pgtol : float\n",
      " |                  A stop condition that uses the projected gradient.\n",
      " |              epsilon\n",
      " |                  If fprime is approximated, use this value for the step\n",
      " |                  size. Only relevant if LikelihoodModel.score is None.\n",
      " |              maxfun : int\n",
      " |                  Maximum number of function evaluations to make.\n",
      " |              bounds : sequence\n",
      " |                  (min, max) pairs for each element in x,\n",
      " |                  defining the bounds on that parameter.\n",
      " |                  Use None for one of min or max when there is no bound\n",
      " |                  in that direction.\n",
      " |          'cg'\n",
      " |              gtol : float\n",
      " |                  Stop when norm of gradient is less than gtol.\n",
      " |              norm : float\n",
      " |                  Order of norm (np.Inf is max, -np.Inf is min)\n",
      " |              epsilon : float\n",
      " |                  If fprime is approximated, use this value for the step\n",
      " |                  size. Can be scalar or vector.  Only relevant if\n",
      " |                  Likelihoodmodel.score is None.\n",
      " |          'ncg'\n",
      " |              fhess_p : callable f'(x,*args)\n",
      " |                  Function which computes the Hessian of f times an arbitrary\n",
      " |                  vector, p.  Should only be supplied if\n",
      " |                  LikelihoodModel.hessian is None.\n",
      " |              avextol : float\n",
      " |                  Stop when the average relative error in the minimizer\n",
      " |                  falls below this amount.\n",
      " |              epsilon : float or ndarray\n",
      " |                  If fhess is approximated, use this value for the step size.\n",
      " |                  Only relevant if Likelihoodmodel.hessian is None.\n",
      " |          'powell'\n",
      " |              xtol : float\n",
      " |                  Line-search error tolerance\n",
      " |              ftol : float\n",
      " |                  Relative error in loglike(params) for acceptable for\n",
      " |                  convergence.\n",
      " |              maxfun : int\n",
      " |                  Maximum number of function evaluations to make.\n",
      " |              start_direc : ndarray\n",
      " |                  Initial direction set.\n",
      " |          'basinhopping'\n",
      " |              niter : int\n",
      " |                  The number of basin hopping iterations.\n",
      " |              niter_success : int\n",
      " |                  Stop the run if the global minimum candidate remains the\n",
      " |                  same for this number of iterations.\n",
      " |              T : float\n",
      " |                  The \"temperature\" parameter for the accept or reject\n",
      " |                  criterion. Higher \"temperatures\" mean that larger jumps\n",
      " |                  in function value will be accepted. For best results\n",
      " |                  `T` should be comparable to the separation (in function\n",
      " |                  value) between local minima.\n",
      " |              stepsize : float\n",
      " |                  Initial step size for use in the random displacement.\n",
      " |              interval : int\n",
      " |                  The interval for how often to update the `stepsize`.\n",
      " |              minimizer : dict\n",
      " |                  Extra keyword arguments to be passed to the minimizer\n",
      " |                  `scipy.optimize.minimize()`, for example 'method' - the\n",
      " |                  minimization method (e.g. 'L-BFGS-B'), or 'tol' - the\n",
      " |                  tolerance for termination. Other arguments are mapped from\n",
      " |                  explicit argument of `fit`:\n",
      " |                    - `args` <- `fargs`\n",
      " |                    - `jac` <- `score`\n",
      " |                    - `hess` <- `hess`\n",
      " |          'minimize'\n",
      " |              min_method : str, optional\n",
      " |                  Name of minimization method to use.\n",
      " |                  Any method specific arguments can be passed directly.\n",
      " |                  For a list of methods and their arguments, see\n",
      " |                  documentation of `scipy.optimize.minimize`.\n",
      " |                  If no method is specified, then BFGS is used.\n",
      " |  \n",
      " |  hessian(self, params)\n",
      " |      Logit model Hessian matrix of the log-likelihood\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The parameters of the model\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      hess : ndarray, (k_vars, k_vars)\n",
      " |          The Hessian, second derivative of loglikelihood function,\n",
      " |          evaluated at `params`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. math:: \\frac{\\partial^{2}\\ln L}{\\partial\\beta\\partial\\beta^{\\prime}}=-\\sum_{i}\\Lambda_{i}\\left(1-\\Lambda_{i}\\right)x_{i}x_{i}^{\\prime}\n",
      " |  \n",
      " |  loglike(self, params)\n",
      " |      Log-likelihood of logit model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The parameters of the logit model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      loglike : float\n",
      " |          The log-likelihood function of the model evaluated at `params`.\n",
      " |          See notes.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. math::\n",
      " |      \n",
      " |         \\ln L=\\sum_{i}\\ln\\Lambda\n",
      " |         \\left(q_{i}x_{i}^{\\prime}\\beta\\right)\n",
      " |      \n",
      " |      Where :math:`q=2y-1`. This simplification comes from the fact that the\n",
      " |      logistic distribution is symmetric.\n",
      " |  \n",
      " |  loglikeobs(self, params)\n",
      " |      Log-likelihood of logit model for each observation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The parameters of the logit model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      loglike : ndarray\n",
      " |          The log likelihood for each observation of the model evaluated\n",
      " |          at `params`. See Notes\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. math::\n",
      " |      \n",
      " |         \\ln L=\\sum_{i}\\ln\\Lambda\n",
      " |         \\left(q_{i}x_{i}^{\\prime}\\beta\\right)\n",
      " |      \n",
      " |      for observations :math:`i=1,...,n`\n",
      " |      \n",
      " |      where :math:`q=2y-1`. This simplification comes from the fact that the\n",
      " |      logistic distribution is symmetric.\n",
      " |  \n",
      " |  pdf(self, X)\n",
      " |      The logistic probability density function\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          `X` is the linear predictor of the logit model.  See notes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pdf : ndarray\n",
      " |          The value of the Logit probability mass function, PMF, for each\n",
      " |          point of X. ``np.exp(-x)/(1+np.exp(-X))**2``\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      In the logit model,\n",
      " |      \n",
      " |      .. math:: \\lambda\\left(x^{\\prime}\\beta\\right)=\\frac{e^{-x^{\\prime}\\beta}}{\\left(1+e^{-x^{\\prime}\\beta}\\right)^{2}}\n",
      " |  \n",
      " |  score(self, params)\n",
      " |      Logit model score (gradient) vector of the log-likelihood\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The parameters of the model\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : ndarray, 1-D\n",
      " |          The score vector of the model, i.e. the first derivative of the\n",
      " |          loglikelihood function, evaluated at `params`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. math:: \\frac{\\partial\\ln L}{\\partial\\beta}=\\sum_{i=1}^{n}\\left(y_{i}-\\Lambda_{i}\\right)x_{i}\n",
      " |  \n",
      " |  score_obs(self, params)\n",
      " |      Logit model Jacobian of the log-likelihood for each observation\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          The parameters of the model\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      jac : array_like\n",
      " |          The derivative of the loglikelihood for each observation evaluated\n",
      " |          at `params`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. math:: \\frac{\\partial\\ln L_{i}}{\\partial\\beta}=\\left(y_{i}-\\Lambda_{i}\\right)x_{i}\n",
      " |      \n",
      " |      for observations :math:`i=1,...,n`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BinaryModel:\n",
      " |  \n",
      " |  __init__(self, endog, exog, check_rank=True, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit_regularized(self, start_params=None, method='l1', maxiter='defined_by_method', full_output=1, disp=1, callback=None, alpha=0, trim_mode='auto', auto_trim_tol=0.01, size_trim_tol=0.0001, qc_tol=0.03, **kwargs)\n",
      " |      Fit the model using a regularized maximum likelihood.\n",
      " |      \n",
      " |      The regularization method AND the solver used is determined by the\n",
      " |      argument method.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      start_params : array_like, optional\n",
      " |          Initial guess of the solution for the loglikelihood maximization.\n",
      " |          The default is an array of zeros.\n",
      " |      method : 'l1' or 'l1_cvxopt_cp'\n",
      " |          See notes for details.\n",
      " |      maxiter : {int, 'defined_by_method'}\n",
      " |          Maximum number of iterations to perform.\n",
      " |          If 'defined_by_method', then use method defaults (see notes).\n",
      " |      full_output : bool\n",
      " |          Set to True to have all available output in the Results object's\n",
      " |          mle_retvals attribute. The output is dependent on the solver.\n",
      " |          See LikelihoodModelResults notes section for more information.\n",
      " |      disp : bool\n",
      " |          Set to True to print convergence messages.\n",
      " |      fargs : tuple\n",
      " |          Extra arguments passed to the likelihood function, i.e.,\n",
      " |          loglike(x,*args).\n",
      " |      callback : callable callback(xk)\n",
      " |          Called after each iteration, as callback(xk), where xk is the\n",
      " |          current parameter vector.\n",
      " |      retall : bool\n",
      " |          Set to True to return list of solutions at each iteration.\n",
      " |          Available in Results object's mle_retvals attribute.\n",
      " |      alpha : non-negative scalar or numpy array (same size as parameters)\n",
      " |          The weight multiplying the l1 penalty term.\n",
      " |      trim_mode : 'auto, 'size', or 'off'\n",
      " |          If not 'off', trim (set to zero) parameters that would have been\n",
      " |          zero if the solver reached the theoretical minimum.\n",
      " |          If 'auto', trim params using the Theory above.\n",
      " |          If 'size', trim params if they have very small absolute value.\n",
      " |      size_trim_tol : float or 'auto' (default = 'auto')\n",
      " |          Tolerance used when trim_mode == 'size'.\n",
      " |      auto_trim_tol : float\n",
      " |          Tolerance used when trim_mode == 'auto'.\n",
      " |      qc_tol : float\n",
      " |          Print warning and do not allow auto trim when (ii) (above) is\n",
      " |          violated by this much.\n",
      " |      qc_verbose : bool\n",
      " |          If true, print out a full QC report upon failure.\n",
      " |      **kwargs\n",
      " |          Additional keyword arguments used when fitting the model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Results\n",
      " |          A results instance.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Using 'l1_cvxopt_cp' requires the cvxopt module.\n",
      " |      \n",
      " |      Extra parameters are not penalized if alpha is given as a scalar.\n",
      " |      An example is the shape parameter in NegativeBinomial `nb1` and `nb2`.\n",
      " |      \n",
      " |      Optional arguments for the solvers (available in Results.mle_settings)::\n",
      " |      \n",
      " |          'l1'\n",
      " |              acc : float (default 1e-6)\n",
      " |                  Requested accuracy as used by slsqp\n",
      " |          'l1_cvxopt_cp'\n",
      " |              abstol : float\n",
      " |                  absolute accuracy (default: 1e-7).\n",
      " |              reltol : float\n",
      " |                  relative accuracy (default: 1e-6).\n",
      " |              feastol : float\n",
      " |                  tolerance for feasibility conditions (default: 1e-7).\n",
      " |              refinement : int\n",
      " |                  number of iterative refinement steps when solving KKT\n",
      " |                  equations (default: 1).\n",
      " |      \n",
      " |      Optimization methodology\n",
      " |      \n",
      " |      With :math:`L` the negative log likelihood, we solve the convex but\n",
      " |      non-smooth problem\n",
      " |      \n",
      " |      .. math:: \\min_\\beta L(\\beta) + \\sum_k\\alpha_k |\\beta_k|\n",
      " |      \n",
      " |      via the transformation to the smooth, convex, constrained problem\n",
      " |      in twice as many variables (adding the \"added variables\" :math:`u_k`)\n",
      " |      \n",
      " |      .. math:: \\min_{\\beta,u} L(\\beta) + \\sum_k\\alpha_k u_k,\n",
      " |      \n",
      " |      subject to\n",
      " |      \n",
      " |      .. math:: -u_k \\leq \\beta_k \\leq u_k.\n",
      " |      \n",
      " |      With :math:`\\partial_k L` the derivative of :math:`L` in the\n",
      " |      :math:`k^{th}` parameter direction, theory dictates that, at the\n",
      " |      minimum, exactly one of two conditions holds:\n",
      " |      \n",
      " |      (i) :math:`|\\partial_k L| = \\alpha_k`  and  :math:`\\beta_k \\neq 0`\n",
      " |      (ii) :math:`|\\partial_k L| \\leq \\alpha_k`  and  :math:`\\beta_k = 0`\n",
      " |  \n",
      " |  predict(self, params, exog=None, linear=False)\n",
      " |      Predict response variable of a model given exogenous variables.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array_like\n",
      " |          Fitted parameters of the model.\n",
      " |      exog : array_like\n",
      " |          1d or 2d array of exogenous values.  If not supplied, the\n",
      " |          whole exog attribute of the model is used.\n",
      " |      linear : bool, optional\n",
      " |          If True, returns the linear predictor dot(exog,params).  Else,\n",
      " |          returns the value of the cdf at the linear predictor.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array\n",
      " |          Fitted values at exog.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from DiscreteModel:\n",
      " |  \n",
      " |  cov_params_func_l1(self, likelihood_model, xopt, retvals)\n",
      " |      Computes cov_params on a reduced parameter space\n",
      " |      corresponding to the nonzero parameters resulting from the\n",
      " |      l1 regularized fit.\n",
      " |      \n",
      " |      Returns a full cov_params matrix, with entries corresponding\n",
      " |      to zero'd values set to np.nan.\n",
      " |  \n",
      " |  initialize(self)\n",
      " |      Initialize is called by\n",
      " |      statsmodels.model.LikelihoodModel.__init__\n",
      " |      and should contain any preprocessing that needs to be done for a model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from statsmodels.base.model.LikelihoodModel:\n",
      " |  \n",
      " |  information(self, params)\n",
      " |      Fisher information matrix of model.\n",
      " |      \n",
      " |      Returns -1 * Hessian of the log-likelihood evaluated at params.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : ndarray\n",
      " |          The model parameters.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from statsmodels.base.model.Model:\n",
      " |  \n",
      " |  from_formula(formula, data, subset=None, drop_cols=None, *args, **kwargs) from builtins.type\n",
      " |      Create a Model from a formula and dataframe.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      formula : str or generic Formula object\n",
      " |          The formula specifying the model.\n",
      " |      data : array_like\n",
      " |          The data for the model. See Notes.\n",
      " |      subset : array_like\n",
      " |          An array-like object of booleans, integers, or index values that\n",
      " |          indicate the subset of df to use in the model. Assumes df is a\n",
      " |          `pandas.DataFrame`.\n",
      " |      drop_cols : array_like\n",
      " |          Columns to drop from the design matrix.  Cannot be used to\n",
      " |          drop terms involving categoricals.\n",
      " |      *args\n",
      " |          Additional positional argument that are passed to the model.\n",
      " |      **kwargs\n",
      " |          These are passed to the model with one exception. The\n",
      " |          ``eval_env`` keyword is passed to patsy. It can be either a\n",
      " |          :class:`patsy:patsy.EvalEnvironment` object or an integer\n",
      " |          indicating the depth of the namespace to use. For example, the\n",
      " |          default ``eval_env=0`` uses the calling namespace. If you wish\n",
      " |          to use a \"clean\" environment set ``eval_env=-1``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      model\n",
      " |          The model instance.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      data must define __getitem__ with the keys in the formula terms\n",
      " |      args and kwargs are passed on to the model instantiation. E.g.,\n",
      " |      a numpy structured or rec array, a dictionary, or a pandas DataFrame.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from statsmodels.base.model.Model:\n",
      " |  \n",
      " |  endog_names\n",
      " |      Names of endogenous variables.\n",
      " |  \n",
      " |  exog_names\n",
      " |      Names of exogenous variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from statsmodels.base.model.Model:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 関数の情報を確認\n",
    "help(sm.Logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GPA</th>\n",
       "      <th>試験成績</th>\n",
       "      <th>プログラム参加</th>\n",
       "      <th>評価向上</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.67</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.65</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.00</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3.10</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2.39</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     GPA  試験成績  プログラム参加  評価向上\n",
       "27  2.67    24        1     0\n",
       "28  3.65    21        1     1\n",
       "29  4.00    23        1     1\n",
       "30  3.10    21        1     0\n",
       "31  2.39    19        1     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spector = pd.read_csv('./data/spector.csv')\n",
    "x = sm.add_constant(spector.iloc[:, :3])\n",
    "y = spector['評価向上']\n",
    "spector.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.402801\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>評価向上</td>       <th>  No. Observations:  </th>  <td>    32</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>    28</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Tue, 18 Jan 2022</td> <th>  Pseudo R-squ.:     </th>  <td>0.3740</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>10:08:55</td>     <th>  Log-Likelihood:    </th> <td> -12.890</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -20.592</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>0.001502</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>  -13.0213</td> <td>    4.931</td> <td>   -2.641</td> <td> 0.008</td> <td>  -22.687</td> <td>   -3.356</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GPA</th>     <td>    2.8261</td> <td>    1.263</td> <td>    2.238</td> <td> 0.025</td> <td>    0.351</td> <td>    5.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>試験成績</th>    <td>    0.0952</td> <td>    0.142</td> <td>    0.672</td> <td> 0.501</td> <td>   -0.182</td> <td>    0.373</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>プログラム参加</th> <td>    2.3787</td> <td>    1.065</td> <td>    2.234</td> <td> 0.025</td> <td>    0.292</td> <td>    4.465</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                   評価向上   No. Observations:                   32\n",
       "Model:                          Logit   Df Residuals:                       28\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Tue, 18 Jan 2022   Pseudo R-squ.:                  0.3740\n",
       "Time:                        10:08:55   Log-Likelihood:                -12.890\n",
       "converged:                       True   LL-Null:                       -20.592\n",
       "Covariance Type:            nonrobust   LLR p-value:                  0.001502\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        -13.0213      4.931     -2.641      0.008     -22.687      -3.356\n",
       "GPA            2.8261      1.263      2.238      0.025       0.351       5.301\n",
       "試験成績           0.0952      0.142      0.672      0.501      -0.182       0.373\n",
       "プログラム参加        2.3787      1.065      2.234      0.025       0.292       4.465\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = sm.Logit(y, x)\n",
    "fit1 = model1.fit()\n",
    "fit1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 評価向上に値するか否かをロジスティック回帰した結果、（$p = 0.05$）\n",
    "    - 定数項: 統計的に優位\n",
    "    - GPA: 統計的に優位\n",
    "    - 試験成績: **統計的に優位でない**\n",
    "    - プログラム参加: 統計的に優位"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- この場合、統計的に優位でない独立変数を減らし、再度回帰を行うことで、合理的な予測値を得る。\n",
    "\n",
    "```\n",
    "# 2列目の変数を除き、独立変数に格納する。\n",
    "spector = pd.read_csv('./data/spector.csv')\n",
    "x = sm.add_constant(spector.iloc[:, [1, 3]]) # 1, 3列のみ\n",
    "y = spector['評価向上']\n",
    "spector.tail()\n",
    "```\n",
    "\n",
    "```\n",
    "# ロジットモデルの実行\n",
    "model2 = sm.Logit(y, x)\n",
    "fit2 = model2.fit()\n",
    "fit2.summary()\n",
    "```\n",
    "\n",
    "- 今回は、このままモデルを実行すると\"Perfect separation detected, results not available\"のエラーとなるため、これ以上の続行は不可能である。\n",
    "    - 独立変数\"プログラム参加\"の値と目的変数\"評価向上\"の値がほとんど一致してしまっているために生じたエラーである。\n",
    "    - サンプルデータのデータ数や独立変数が少なすぎるため、エラーが発生した。\n",
    "- *後日、もっとわかりやすい別データに差し替え予定*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （参考）その他の記述方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>評価向上</td>       <th>  No. Observations:  </th>  <td>    32</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>    28</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>         <td>Logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -12.890</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Tue, 18 Jan 2022</td> <th>  Deviance:          </th> <td>  25.779</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>10:15:26</td>     <th>  Pearson chi2:      </th>  <td>  27.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>5</td>        <th>  Pseudo R-squ. (CS):</th>  <td>0.3821</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>  -13.0213</td> <td>    4.931</td> <td>   -2.641</td> <td> 0.008</td> <td>  -22.686</td> <td>   -3.356</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GPA</th>     <td>    2.8261</td> <td>    1.263</td> <td>    2.238</td> <td> 0.025</td> <td>    0.351</td> <td>    5.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>試験成績</th>    <td>    0.0952</td> <td>    0.142</td> <td>    0.672</td> <td> 0.501</td> <td>   -0.182</td> <td>    0.373</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>プログラム参加</th> <td>    2.3787</td> <td>    1.065</td> <td>    2.234</td> <td> 0.025</td> <td>    0.292</td> <td>    4.465</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                   評価向上   No. Observations:                   32\n",
       "Model:                            GLM   Df Residuals:                       28\n",
       "Model Family:                Binomial   Df Model:                            3\n",
       "Link Function:                  Logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -12.890\n",
       "Date:                Tue, 18 Jan 2022   Deviance:                       25.779\n",
       "Time:                        10:15:26   Pearson chi2:                     27.3\n",
       "No. Iterations:                     5   Pseudo R-squ. (CS):             0.3821\n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        -13.0213      4.931     -2.641      0.008     -22.686      -3.356\n",
       "GPA            2.8261      1.263      2.238      0.025       0.351       5.301\n",
       "試験成績           0.0952      0.142      0.672      0.501      -0.182       0.373\n",
       "プログラム参加        2.3787      1.065      2.234      0.025       0.292       4.465\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# （参考） GLM (Generalized Linear Models) を使う記述方法\n",
    "# ロジットモデルは、リンク関数にLogit()を用いる一般化回帰モデル(GLM)の一種であるので sm.GLMを使って以下のようにも書くことができる。（当然、結果は一緒である。）\n",
    "    # 関数の詳細は以下のコマンドで確認できる。\n",
    "    # help(sm.GLM)\n",
    "# families.Binomial()は yの値が二項分布であるという意味である。\n",
    "model2 = sm.GLM(y, x, family=sm.families.Binomial()) \n",
    "fit2 = model2.fit()\n",
    "fit2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.402801\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>評価向上</td>       <th>  No. Observations:  </th>  <td>    32</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>    28</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Tue, 18 Jan 2022</td> <th>  Pseudo R-squ.:     </th>  <td>0.3740</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>10:17:19</td>     <th>  Log-Likelihood:    </th> <td> -12.890</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -20.592</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>0.001502</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>  -13.0213</td> <td>    4.931</td> <td>   -2.641</td> <td> 0.008</td> <td>  -22.687</td> <td>   -3.356</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GPA</th>       <td>    2.8261</td> <td>    1.263</td> <td>    2.238</td> <td> 0.025</td> <td>    0.351</td> <td>    5.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>試験成績</th>      <td>    0.0952</td> <td>    0.142</td> <td>    0.672</td> <td> 0.501</td> <td>   -0.182</td> <td>    0.373</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>プログラム参加</th>   <td>    2.3787</td> <td>    1.065</td> <td>    2.234</td> <td> 0.025</td> <td>    0.292</td> <td>    4.465</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                   評価向上   No. Observations:                   32\n",
       "Model:                          Logit   Df Residuals:                       28\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Tue, 18 Jan 2022   Pseudo R-squ.:                  0.3740\n",
       "Time:                        10:17:19   Log-Likelihood:                -12.890\n",
       "converged:                       True   LL-Null:                       -20.592\n",
       "Covariance Type:            nonrobust   LLR p-value:                  0.001502\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept    -13.0213      4.931     -2.641      0.008     -22.687      -3.356\n",
       "GPA            2.8261      1.263      2.238      0.025       0.351       5.301\n",
       "試験成績           0.0952      0.142      0.672      0.501      -0.182       0.373\n",
       "プログラム参加        2.3787      1.065      2.234      0.025       0.292       4.465\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# （参考） formula 形式: Rと同じように記述できる方式\n",
    "formula = '評価向上~{}'.format('+'.join(spector.columns[:3]))\n",
    "model3 = smf.logit(formula, data=spector)\n",
    "fit3 = model3.fit()\n",
    "fit3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>評価向上</td>       <th>  No. Observations:  </th>  <td>    32</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>    28</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>         <td>Logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -12.890</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Tue, 18 Jan 2022</td> <th>  Deviance:          </th> <td>  25.779</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>10:17:46</td>     <th>  Pearson chi2:      </th>  <td>  27.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>5</td>        <th>  Pseudo R-squ. (CS):</th>  <td>0.3821</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>  -13.0213</td> <td>    4.931</td> <td>   -2.641</td> <td> 0.008</td> <td>  -22.686</td> <td>   -3.356</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GPA</th>       <td>    2.8261</td> <td>    1.263</td> <td>    2.238</td> <td> 0.025</td> <td>    0.351</td> <td>    5.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>試験成績</th>      <td>    0.0952</td> <td>    0.142</td> <td>    0.672</td> <td> 0.501</td> <td>   -0.182</td> <td>    0.373</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>プログラム参加</th>   <td>    2.3787</td> <td>    1.065</td> <td>    2.234</td> <td> 0.025</td> <td>    0.292</td> <td>    4.465</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:                   評価向上   No. Observations:                   32\n",
       "Model:                            GLM   Df Residuals:                       28\n",
       "Model Family:                Binomial   Df Model:                            3\n",
       "Link Function:                  Logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -12.890\n",
       "Date:                Tue, 18 Jan 2022   Deviance:                       25.779\n",
       "Time:                        10:17:46   Pearson chi2:                     27.3\n",
       "No. Iterations:                     5   Pseudo R-squ. (CS):             0.3821\n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept    -13.0213      4.931     -2.641      0.008     -22.686      -3.356\n",
       "GPA            2.8261      1.263      2.238      0.025       0.351       5.301\n",
       "試験成績           0.0952      0.142      0.672      0.501      -0.182       0.373\n",
       "プログラム参加        2.3787      1.065      2.234      0.025       0.292       4.465\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# （参考） formula形式かつ、GLMを利用した書き方\n",
    "model4 = smf.glm(formula, data=spector, family=sm.families.Binomial())\n",
    "fit4 = model4.fit()\n",
    "fit4.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}